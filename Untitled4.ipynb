{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORU6JNS5lAp53ikxqJKCWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeyoun/sum_artificial/blob/masin/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ORjInsgD7vyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071aec7d-ab83-405a-c935-fe0110345bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy is:\n",
            "(0,0): terminal\n",
            "(0,1): up\n",
            "(0,2): up\n",
            "(0,3): up\n",
            "(1,0): up\n",
            "(1,1): up\n",
            "(1,2): up\n",
            "(1,3): up\n",
            "(2,0): up\n",
            "(2,1): up\n",
            "(2,2): up\n",
            "(2,3): up\n",
            "(3,0): up\n",
            "(3,1): up\n",
            "(3,2): up\n",
            "(3,3): terminal\n",
            "\n",
            "Optimal value function is:\n",
            "V(0,0): 0.0\n",
            "V(0,1): 0.0\n",
            "V(0,2): 0.0\n",
            "V(0,3): 0.0\n",
            "V(1,0): 0.0\n",
            "V(1,1): 0.0\n",
            "V(1,2): 0.0\n",
            "V(1,3): 0.0\n",
            "V(2,0): 0.0\n",
            "V(2,1): 0.0\n",
            "V(2,2): 0.0\n",
            "V(2,3): 0.0\n",
            "V(3,0): 0.0\n",
            "V(3,1): 0.0\n",
            "V(3,2): 0.0\n",
            "V(3,3): 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# 1. MDP 정의\n",
        "# (1) S\n",
        "# shape = (4, 4)\n",
        "terminals = [(0, 0), (3, 3)]\n",
        "# (2) A\n",
        "numa = 4\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# 4x4 매트릭스 선언 및 초기화\n",
        "shape = (4, 4)\n",
        "\n",
        "# 생성된 매트릭스 출력\n",
        "# print(matrix_4x4)\n",
        "\n",
        "# 1. MDP 정의\n",
        "# (3) P\n",
        "\n",
        "# reward_up = -1 * np.ones(shape)\n",
        "# reward_down = -2 * np.ones(shape)\n",
        "# reward_left = -3 * np.ones(shape)\n",
        "# reward_right = -3 * np.ones(shape)\n",
        "\n",
        "# for terminal in terminals:\n",
        "#   reward_up[terminal] = 0\n",
        "#   reward_down[terminal] = 0\n",
        "#   reward_left[terminal] = 0\n",
        "#   reward_right[terminal] = 0\n",
        "\n",
        "def P(state, action):\n",
        "  if action == 'up':\n",
        "    next_state = (max(0, state[0]-1), state[1])\n",
        "  elif action == 'down':\n",
        "    next_state = (min(shape[0]-1, state[0]+1), state[1])\n",
        "  elif action == 'left':\n",
        "    next_state = (state[0], max(0, state[1]-1))\n",
        "  elif action == 'right':\n",
        "    next_state = (state[0], min(shape[1]-1, state[1]+1))\n",
        "  return next_state\n",
        "\n",
        "# (4) R\n",
        "reward = -1 * np.ones(shape)\n",
        "for terminal in terminals:\n",
        "  reward[terminal] = 0\n",
        "\n",
        "# (5) gamma\n",
        "gamma = 1.0\n",
        "\n",
        "# 2. value iteration\n",
        "# (1) Initialize the value function\n",
        "V = np.zeros(shape)\n",
        "\n",
        "#(2) Value iteration\n",
        "while True:\n",
        "  delta = 0\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      if(i,j) in terminals:\n",
        "        continue\n",
        "      v= V[i,j]\n",
        "      V[i,j] = sum ((reward[i,j]+gamma + V[P((i,j),a)]) for a in actions) / numa\n",
        "      delta = max(delta, abs(v - V[i,j]))\n",
        "  if delta < 1e-4:\n",
        "    break\n",
        "\n",
        "#2 2. value iteration\n",
        "# (3) Extract the optimal policy\n",
        "optimal_policy = {}\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "    if (i,j) in terminals:\n",
        "      optimal_policy[i,j]='terminal'\n",
        "    else:\n",
        "      optimal_policy[i,j]=actions[np.argmax([(reward[i,j]+gamma*V[P((i,j), a)]) for a in actions])]\n",
        "\n",
        "#3. 결과 출력\n",
        "print(\"Optimal policy is:\")\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "    print(f\"({i},{j}): {optimal_policy[i,j]}\")\n",
        "\n",
        "print(\"\\nOptimal value function is:\")\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "    print(f\"V({i},{j}): {V[i,j]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# 1. MDP 정의\n",
        "# (1) S\n",
        "# shape = (4, 4)\n",
        "terminals = [(0, 0), (3, 3)]\n",
        "# (2) A\n",
        "numa = 4\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# 4x4 매트릭스 선언 및 초기화\n",
        "shape = (4, 4)\n",
        "\n",
        "# 생성된 매트릭스 출력\n",
        "# print(matrix_4x4)\n",
        "\n",
        "# 1. MDP 정의\n",
        "# (3) P\n",
        "\n",
        "# reward_up = -1 * np.ones(shape)\n",
        "# reward_down = -2 * np.ones(shape)\n",
        "# reward_left = -3 * np.ones(shape)\n",
        "# reward_right = -3 * np.ones(shape)\n",
        "\n",
        "# for terminal in terminals:\n",
        "#   reward_up[terminal] = 0\n",
        "#   reward_down[terminal] = 0\n",
        "#   reward_left[terminal] = 0\n",
        "#   reward_right[terminal] = 0\n",
        "\n",
        "def P(state, action):\n",
        "  if action == 'up':\n",
        "    next_state = (max(0, state[0]-1), state[1])\n",
        "  elif action == 'down':\n",
        "    next_state = (min(shape[0]-1, state[0]+1), state[1])\n",
        "  elif action == 'left':\n",
        "    next_state = (state[0], max(0, state[1]-1))\n",
        "  elif action == 'right':\n",
        "    next_state = (state[0], min(shape[1]-1, state[1]+1))\n",
        "  return next_state\n",
        "\n",
        "# (4) R\n",
        "reward = -1 * np.ones(shape)\n",
        "for terminal in terminals:\n",
        "  reward[terminal] = 0\n",
        "\n",
        "# (5) gamma\n",
        "gamma = 1.0\n",
        "\n",
        "# 2. value iteration\n",
        "# (1) Initialize the value function\n",
        "V = np.zeros(shape)\n",
        "\n",
        "policy = {state: np.random.choice(actions) for state in\n",
        "    [(i, j) for i in range(shape[0]) for j in range(shape[1])] if state not in terminals}\n",
        "\n",
        "# Policy iteration\n",
        "while True:\n",
        "    # Policy evaluation\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                if (i, j) in terminals:\n",
        "                    continue\n",
        "                v = V[i, j]\n",
        "                action = policy[(i, j)]\n",
        "                V[i, j] = reward[i, j] + gamma * V[P((i, j), action)]\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "        if delta < 2.01:\n",
        "            break\n",
        "\n",
        "     # Policy improvement\n",
        "    policy_stable = True\n",
        "    for i in range(shape[0]):\n",
        "        for j in range(shape[1]):\n",
        "            if (i, j) in terminals:\n",
        "                continue\n",
        "            old_action = policy[(i, j)]\n",
        "            policy[(i, j)] = actions[np.argmax([reward[i, j]\n",
        "                           + gamma * V[P((i, j), action)] for action in actions])]\n",
        "            if old_action != policy[(i, j)]:\n",
        "                policy_stable = False\n",
        "\n",
        "    if policy_stable:\n",
        "        break\n",
        "\n",
        "\n",
        "#3. 결과 출력\n",
        "print(\"Optimal policy is:\")\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "    print(f\"({i},{j}): {optimal_policy[i,j]}\")\n",
        "\n",
        "print(\"\\nOptimal value function is:\")\n",
        "for i in range(shape[0]):\n",
        "  for j in range(shape[1]):\n",
        "    print(f\"V({i},{j}): {V[i,j]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1Vgc3TzHuR-",
        "outputId": "de3e1384-77f6-40ec-e6f0-fe814b2ceee1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy is:\n",
            "(0,0): terminal\n",
            "(0,1): up\n",
            "(0,2): up\n",
            "(0,3): up\n",
            "(1,0): up\n",
            "(1,1): up\n",
            "(1,2): up\n",
            "(1,3): up\n",
            "(2,0): up\n",
            "(2,1): up\n",
            "(2,2): up\n",
            "(2,3): up\n",
            "(3,0): up\n",
            "(3,1): up\n",
            "(3,2): up\n",
            "(3,3): terminal\n",
            "\n",
            "Optimal value function is:\n",
            "V(0,0): 0.0\n",
            "V(0,1): -1.0\n",
            "V(0,2): -2.0\n",
            "V(0,3): -3.0\n",
            "V(1,0): -1.0\n",
            "V(1,1): -2.0\n",
            "V(1,2): -3.0\n",
            "V(1,3): -2.0\n",
            "V(2,0): -2.0\n",
            "V(2,1): -3.0\n",
            "V(2,2): -2.0\n",
            "V(2,3): -1.0\n",
            "V(3,0): -3.0\n",
            "V(3,1): -2.0\n",
            "V(3,2): -1.0\n",
            "V(3,3): 0.0\n"
          ]
        }
      ]
    }
  ]
}